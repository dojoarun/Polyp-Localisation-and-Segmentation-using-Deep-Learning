{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"2-Spit_Dataset.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"HTCjnuBJRAQX","colab_type":"text"},"source":["## Split polyps / non-polyps dataset\n","\n","Use `polyps` and `non_polyps` from `cropped` to copy files into `train` and `validation` from `data_polyps`. This folder should already exists and have both subfolders (or ajust the script to create them). The current **data_polyps** folder will be the dataset folder for the next deep learning classifications.\n","\n","Therefore, we shall use a dataset split percentage such as **75% train** and **25% test**.\n","\n","We need `os` and `shutil` to manage the files, `random` to randomly split the dataset in train and validation subsets. You should have a folder structure such as:\n","\n","* `./data_polyps/train/polyps`\n","* `./data_polyps/train/non_polyps`\n","* `./data_polyps/validation/polyps`\n","* `./data_polyps/validation/non_polyps`"]},{"cell_type":"code","metadata":{"id":"kWWK0wjqRAQg","colab_type":"code","colab":{}},"source":["import os\n","import random\n","import shutil"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2-6NObfIRLZt","colab_type":"code","outputId":"1cf7fe87-5ea0-4910-80c9-3ec067551660","executionInfo":{"status":"ok","timestamp":1584611795744,"user_tz":-330,"elapsed":66668,"user":{"displayName":"Arun Sreenivas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJj1dXpdRFYwseVE4F-1AEmUBPkQVPNAtCKRKq=s64","userId":"16731910840222813526"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XWNuu563RAQw","colab_type":"text"},"source":["These lines are the locations for the source files (the entire dataset) and the future locations of the splitted dataset in train and validation subsets:"]},{"cell_type":"code","metadata":{"id":"C2R5hkkcRAQz","colab_type":"code","colab":{}},"source":["# Source dataset: from where to copy the files\n","sourceFolderClass1 = '/content/drive/My Drive/Colab Notebooks/dataset/cropped/polyps'\n","sourceFolderClass2 = '/content/drive/My Drive/Colab Notebooks/dataset/cropped/polyps'\n","# Destination folders: splitted dataset in train and validation for polyps and non-polyps\n","destFolderClass1_tr  = '/content/drive/My Drive/Colab Notebooks/dataset/train/polyps'\n","destFolderClass2_tr  = '/content/drive/My Drive/Colab Notebooks/dataset/train/non_polyps'\n","destFolderClass1_val = '/content/drive/My Drive/Colab Notebooks/dataset/validation/polyps'\n","destFolderClass2_val = '/content/drive/My Drive/Colab Notebooks/dataset/validation/non_polyps'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2AftzqZIRAQ-","colab_type":"text"},"source":["Get the list with all the files in the source folder:"]},{"cell_type":"code","metadata":{"id":"hHHgyD_JRARB","colab_type":"code","outputId":"9fd79b51-cdd4-4afc-c4c2-dc6fb0c761cc","executionInfo":{"status":"ok","timestamp":1584611830929,"user_tz":-330,"elapsed":4793,"user":{"displayName":"Arun Sreenivas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJj1dXpdRFYwseVE4F-1AEmUBPkQVPNAtCKRKq=s64","userId":"16731910840222813526"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["sourceFiles1 = os.listdir(sourceFolderClass1)\n","sourceFiles2 = os.listdir(sourceFolderClass2)\n","print(\"Class 1 - polyps:\", len(sourceFiles1))\n","print(\"Class 2 - non-polyps:\", len(sourceFiles2))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Class 1 - polyps: 606\n","Class 2 - non-polyps: 606\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"q8zwoLbCRARI","colab_type":"text"},"source":["Let's suffle the listw with the source files using a random seed:"]},{"cell_type":"code","metadata":{"id":"JKkXR2MBRARL","colab_type":"code","colab":{}},"source":["random.seed(1)\n","random.shuffle(sourceFiles1)\n","random.shuffle(sourceFiles2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"omTRIc4-RARR","colab_type":"text"},"source":["We shall define a number of files to copy in the `validation` subfolder for each class. If you want a different split, you should modify `val_files`."]},{"cell_type":"code","metadata":{"id":"4BQ1HmdFRARS","colab_type":"code","outputId":"29c2721c-301f-4daa-ab39-9e3b02170ae7","executionInfo":{"status":"ok","timestamp":1584611960394,"user_tz":-330,"elapsed":113034,"user":{"displayName":"Arun Sreenivas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJj1dXpdRFYwseVE4F-1AEmUBPkQVPNAtCKRKq=s64","userId":"16731910840222813526"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# No of file to copy in VALIDATION folder for each class\n","val_files = 151\n","\n","# Copy the first 151 files for polyps and non-polyps into validation folders\n","print('--> Validation split ...')\n","for i in range(val_files):\n","    # copy validation polyps\n","    File1 = os.path.join(sourceFolderClass1,sourceFiles1[i])\n","    File2 = os.path.join(destFolderClass1_val,  sourceFiles1[i])\n","    shutil.copy(File1,File2)\n","    # copy validation non-polyps\n","    File1 = os.path.join(sourceFolderClass2, sourceFiles2[i])\n","    File2 = os.path.join(destFolderClass2_val,   sourceFiles2[i])\n","    shutil.copy(File1, File2)\n","\n","print('--> Done!')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["--> Validation split ...\n","--> Done!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FnedtyVWRARa","colab_type":"code","outputId":"a4ec9165-d580-476a-993f-a5caff273a1a","executionInfo":{"status":"ok","timestamp":1584612311725,"user_tz":-330,"elapsed":270773,"user":{"displayName":"Arun Sreenivas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJj1dXpdRFYwseVE4F-1AEmUBPkQVPNAtCKRKq=s64","userId":"16731910840222813526"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# Copy polyps to train\n","print('--> Train split ...')\n","for i in range(val_files,len(sourceFiles1)):\n","    File1 = os.path.join(sourceFolderClass1,  sourceFiles1[i])\n","    File2 = os.path.join(destFolderClass1_tr, sourceFiles1[i])\n","    shutil.copy(File1,File2)\n","# copy non-polyps to train\n","for i in range(val_files,len(sourceFiles2)):    \n","    File1 = os.path.join(sourceFolderClass2,  sourceFiles2[i])\n","    File2 = os.path.join(destFolderClass2_tr, sourceFiles2[i])\n","    shutil.copy(File1, File2)\n","\n","print('--> Done!')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["--> Train split ...\n","--> Done!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"B8BbUZjCRARf","colab_type":"text"},"source":["Now we have a splitted dataset into train and validation subfolder with each class inside:\n","* **1212** images in the entire dataset;\n","* **910** images for training: 455 polyps + 455 non-polyps;\n","* **302** images for validation: 151 polyps + 151 non-polyps.\n","\n","Let's check the composition of the subsets for the future classification:"]},{"cell_type":"code","metadata":{"id":"o7ry1m27RARh","colab_type":"code","outputId":"179c1f58-e666-478c-dece-bbb430b4d6f4","executionInfo":{"status":"ok","timestamp":1584615170651,"user_tz":-330,"elapsed":920,"user":{"displayName":"Arun Sreenivas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJj1dXpdRFYwseVE4F-1AEmUBPkQVPNAtCKRKq=s64","userId":"16731910840222813526"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["print('--> Dataset: data_polyps')\n","print('> Train - polyps:', len(os.listdir(destFolderClass1_tr)))\n","print('> Train - non-polyps:', len(os.listdir(destFolderClass2_tr)))\n","print('> Validation - polyps:', len(os.listdir(destFolderClass1_val)))\n","print('> Validation - non-polyps:', len(os.listdir(destFolderClass2_val)))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["--> Dataset: data_polyps\n","> Train - polyps: 569\n","> Train - non-polyps: 565\n","> Validation - polyps: 265\n","> Validation - non-polyps: 261\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TiJZrzNNRARm","colab_type":"text"},"source":["We are ready to use Deep Learning to find a classifier for polyps/non-polyps images. Remember you could modify the dataset splitting, remove manually a specific list of files, use different names for the folders, etc.\n","\n","Let's create some classifiers with the next script [3-Small_CNNs.ipynb](./3-Small_CNNs.ipynb).\n","\n","Have fun with DL! @muntisa"]},{"cell_type":"markdown","metadata":{"id":"yTQyu-tWRARn","colab_type":"text"},"source":["### Acknowledgements\n","\n","I gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research ([https://developer.nvidia.com/academic_gpu_seeding](https://developer.nvidia.com/academic_gpu_seeding))."]},{"cell_type":"code","metadata":{"id":"KKOKGXPDRARo","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}